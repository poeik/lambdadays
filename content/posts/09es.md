---
title:   Keynote AI for Mathematical Discovery Symbolic, Neural and Neuro-Symbolic Methods
date:    2025-06-12
tags:    [lambdadays]
aliases: []
---

Talk held by: Moa Johannson

> How can we learn to construct new properties for functional programming or
> mathematical theorem?

> AI as mathematical co-pilot

- already in 1958 Newell & Simon said, that AI will discover and prove new
  mathematical theorem
- Terence Tao (2023) believes that we will reach that in 2026 using LLMs


## Proof assistants

- formalizations take a lot of time
- proof assistance rely heavily on mathematics libraries
- steep learning curve

=> Very difficult to do

## AI for formalising mathematics

### Autoformalisation

- can we translate proofs and definitions in English to proof assistant
  formalisations using LLMs?
- how do we know that they are correct?

> It is more difficult to translate from English to lean, than from English to
> French.

### Theory exploration/conjecturing

- Given a set of formal definitions (or functions), can we assist with invention
  of conjectures about these?
- We want to find new, interesting and non-trivial

# Finding conjectures with AI

## Symbolic methods (heuristic/search based)

- heuristic search using grammars, rules
- around since 1976 (AM by Lenat)
- complex collection of heuristic rules to find properties of maths
- HR(L) (2000, Coltion/Pease) was a system that found a new integer sequence
  using such a method
- IsaCoSy (Johannson 2011): Automation of proofs by induction
    - out of that [QuickSpec](https://hackage.haskell.org/package/quickspec) came out: it generates properties of a Haskell
      program and tests them using QuickCheck
      - QuickSpec is limited to equations
- RoughSpec:
    - More widely applicable while avoiding exponential search space?
    - Core idea: discovery by analogy of other lemmas
    - You use templates of lemmas, with "holes" synthesise instantiations for
      holes

## Neuron methods

- use LLMs
- How well does ChatGPT what QuickSpec does?
    - Tested with Hendersons's functional geometry library
        - It allows to edit images (like flipping, rotating, etc)
        - e.g. `rot(rot(rot(rot d))) = d` with an image `d`
        --insert bunny picture here--
    - many conjectures were false => not a problem: you can check them by
      theorem prover / counter example checker
    - it used some templates to generate the conjectures

### Specialised neural method

> Use models specialized to find conjectures

- e.g. AlphaGeometry
    - Solves geometric math problems
    - it won a silver medal in mathematical olympia in 2024

## Neuro symbolic

> combination of the two other approaches

- can we leverage analogies between mathematical domains to suggest conjectures?
- Can we learn templates from data and have a neural network suggesting such?
- suggest lemmas in a proof assistant: speed up formalisations

### Lemmanaind

![[attachments/1d548cad-9dca-4d03-ae1a-1977a3ab694e~1 1.jpg]]

> Neuro-symbolic conjecturing

It has 2 components:

- Neural engine: learns definitions, spits out templates (deepseek, to run it on
  modest hardware)
- Symbolic engine: templates go to symbolic engine, it instantiates them and
  sends them to the proof engine

#### Results

Compared to conjectures found by human in isabelle:

- Lemmanaid found: 56.6%
- neural only found: 40%
- symbolic only found: 20%

=> It is also already good to find some conjectures helps speed up the work of
the proof engineer.

*Next steps*:

- exploring with bigger LLMs and other proof assistants
- apply counter-example checkers and automated proof tools to conjectures
- Full integration with a proof assistant

## References

- [https://www.lambdadays.org/lambdadays2025/moa-johansson](https://www.lambdadays.org/lambdadays2025/moa-johansson)

